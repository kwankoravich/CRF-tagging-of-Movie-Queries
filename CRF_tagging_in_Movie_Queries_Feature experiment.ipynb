{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sl-NBezioz4s"
   },
   "source": [
    "# CRF sequence tagging for Move Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhJNed-8oz4u"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from nltk.tag import CRFTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5ltieStoz41"
   },
   "outputs": [],
   "source": [
    "def get_raw_data_from_bio_file(fpath):\n",
    "    \"\"\"A simple function to read in from a one-word-per-line BIO\n",
    "    (Beginning, Inside, Outside) tagged corpus, tab separated\n",
    "    and each example sentence/text separated with a blank line.\n",
    "    The data is already tokenized in a simple way.\n",
    "    e.g.:                     \n",
    "    \n",
    "    O\ta\n",
    "    O\tgreat\n",
    "    O\tlunch\n",
    "    O\tspot\n",
    "    O\tbut\n",
    "    B-Hours\topen\n",
    "    I-Hours\ttill\n",
    "    I-Hours\t2\n",
    "    I-Hours\ta\n",
    "    I-Hours\tm\n",
    "    B-Restaurant_Name\tpassims\n",
    "    I-Restaurant_Name\tkitchen\n",
    "    \n",
    "    returns a list of lists of tuples of (word, tag) tuples\n",
    "    \"\"\"\n",
    "    f = open(fpath)\n",
    "    data = []  # the data, a list of lists of (word, tag) tuples\n",
    "    current_sent = []  # data for current sentence/example\n",
    "    for line in f:\n",
    "        if line == \"\\n\":  # each instance has a blank line separating it from next one\n",
    "            # solution\n",
    "            data.append(current_sent)\n",
    "            current_sent = []\n",
    "            continue\n",
    "        line_data = line.strip(\"\\n\").split(\"\\t\")\n",
    "        current_sent.append((line_data[1], line_data[0]))\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQsXSXh4oz46"
   },
   "outputs": [],
   "source": [
    "raw_training_data = get_raw_data_from_bio_file(\"engtrain.bio.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWecFxnfoz4-",
    "outputId": "e4a9f412-7d06-4471-aadc-e83b32c9e00d"
   },
   "outputs": [],
   "source": [
    "# have a look at the first example\n",
    "print(raw_training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b21pBEPoz5D",
    "outputId": "014f998b-988d-4794-ca50-f651a2c9864e"
   },
   "outputs": [],
   "source": [
    "print(len(raw_training_data), \"instances\")\n",
    "print(sum([len(sent) for sent in raw_training_data]), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrQq2_9poz5H"
   },
   "outputs": [],
   "source": [
    "def preProcess(example):\n",
    "    \"\"\"Function takes in list of (word, bio-tag) pairs, e.g.:\n",
    "        [('what', 'O'), ('movies', 'O'), ('star', 'O'), ('bruce', 'B-ACTOR'), ('willis', 'I-ACTOR')]\n",
    "    returns new (token, bio-tag) pairs with preprocessing applied to the words\"\"\"\n",
    "       \n",
    "        # a postagger for use in exercises\n",
    "    posttagger = CRFTagger()\n",
    "    posttagger.set_model_file(\"crf_pos.tagger\")\n",
    "    # example use:\n",
    "    words = [\"john\", \"likes\", \"mary\", \"and\", \"bill\"]\n",
    "   #print(posttagger.tag(words))\n",
    "        # word tokenisation and POS tagging\n",
    "    \n",
    "    new_tokens = []\n",
    "    for i in example:\n",
    "           Li = list(i) #change the type of data to list\n",
    "           Li[0] = word_tokenize(i[0]) # tokenize the data\n",
    "  \n",
    "           try:\n",
    "              [(a,b,)] = posttagger.tag(Li[0]) # use CRFTagger() function, which has been trained before, and store the result in a ,b\n",
    "           \n",
    "           except ValueError:\n",
    "              pass\n",
    "           \n",
    "           \n",
    "           Li[0] = a + '!' + b # create the dat which is \"word!POS_tagging\"\n",
    "          \n",
    "           i = tuple(Li) # change the data type back to tuple\n",
    "           new_tokens.append(i) # add the data in new_tokens\n",
    "    \n",
    "    \n",
    "    preprocessed_example = new_tokens  \n",
    "    \n",
    "    return preprocessed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOvXY8cToz5K"
   },
   "outputs": [],
   "source": [
    "training_data = [preProcess(example) for example in raw_training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gIxWpt2loz5P",
    "outputId": "7b65d712-1567-48cc-af7c-d9399fc22fb3"
   },
   "outputs": [],
   "source": [
    "# check the effect of pre-processing\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_S04ttSXoz5S"
   },
   "outputs": [],
   "source": [
    "_pattern = re.compile(r\"\\d\")  # to recognize numbers/digits\n",
    "\n",
    "# This is the 'out-of-the-box' get_features function from the nltk CRF tagger\n",
    "def get_features(tokens, idx):\n",
    "    \"\"\"\n",
    "    Extract basic features about this word including\n",
    "         - Current Word\n",
    "         - Is Capitalized ?\n",
    "         - Has Punctuation ?\n",
    "         - Has Number ?\n",
    "         - Suffixes up to length 3\n",
    "    Note that : we might include feature over previous word, next word ect.\n",
    "\n",
    "    :return : a list which contains the features\n",
    "    :rtype : list(str)\n",
    "\n",
    "    \"\"\"\n",
    "    token = tokens[idx]\n",
    "    feature_list = []\n",
    "\n",
    "    if not token:\n",
    "        return feature_list\n",
    "\n",
    "    # Capitalization\n",
    "    if token[0].isupper():\n",
    "        feature_list.append(\"CAPITALIZATION\")\n",
    "\n",
    "    # Number\n",
    "    if re.search(_pattern, token) is not None:\n",
    "        feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "    # Punctuation\n",
    "    punc_cat = set([\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"])\n",
    "    if all(unicodedata.category(x) in punc_cat for x in token):\n",
    "        feature_list.append(\"PUNCTUATION\")\n",
    "    \n",
    "    #lemmatization    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "    token = lemmatizer.lemmatize(token)   \n",
    "\n",
    "    # Suffix up to length 5\n",
    "    if len(token) > 1:\n",
    "        feature_list.append(\"SUF_\" + token[-1:])\n",
    "    if len(token) > 2:\n",
    "        feature_list.append(\"SUF_\" + token[-2:])\n",
    "    if len(token) > 3:\n",
    "        feature_list.append(\"SUF_\" + token[-3:])\n",
    "    if len(token) > 4:\n",
    "        feature_list.append(\"SUF_\" + token[-4:])\n",
    "    if len(token) > 5:\n",
    "        feature_list.append(\"SUF_\" + token[-5:])\n",
    "        \n",
    "    # Prefix up to length 5\n",
    "    if len(token) > 1:\n",
    "        feature_list.append(\"PRE_\" + token[:1])\n",
    "    if len(token) > 2:\n",
    "        feature_list.append(\"PRE_\" + token[:2])\n",
    "    if len(token) > 3:\n",
    "        feature_list.append(\"PRE_\" + token[:3])\n",
    "    if len(token) > 4:\n",
    "        feature_list.append(\"PRE_\" + token[:4])\n",
    "    if len(token) > 5:\n",
    "        feature_list.append(\"PRE_\" + token[:5])\n",
    "      \n",
    "    # Split the special character\n",
    "    token_split_word = token.split('!') #split the word with special character and get the word & POS tag\n",
    "    token = token_split_word[0] # store the word in token \n",
    "        \n",
    "    feature_list.append(\"WORD_\" + token)\n",
    "    feature_list.append(\"POS_\" + token_split_word[1]) # add POS tagging\n",
    "    print(feature_list)\n",
    "    return feature_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yGc81WkJ0PB3",
    "outputId": "1d474bf2-cfca-475b-b46d-07f6eb4edc4a"
   },
   "outputs": [],
   "source": [
    "#pip install python-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4IoVF0j2oz5V",
    "outputId": "2d401231-c434-4716-e14e-b235e7faa39b"
   },
   "outputs": [],
   "source": [
    "# Train the CRF BIO-tag tagger\n",
    "TAGGER_PATH = \"crf_nlu.tagger\"  # path to the tagger- it will save/access the model from here\n",
    "ct = CRFTagger(feature_func=get_features)  # initialize tagger with get_features function\n",
    "\n",
    "print(\"training tagger...\")\n",
    "ct.train(training_data, TAGGER_PATH)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2loxKNVRoz5Y",
    "outputId": "3dc23b02-2f01-4c94-80f7-301b09698f9f"
   },
   "outputs": [],
   "source": [
    "# load tagger from saved file\n",
    "ct = CRFTagger(feature_func=get_features)  # initialize tagger\n",
    "ct.set_model_file(TAGGER_PATH)  # load model from file\n",
    "\n",
    "# prepare the test data:\n",
    "raw_test_data = get_raw_data_from_bio_file(\"engtest.bio.txt\") \n",
    "test_data = [preProcess(example) for example in raw_test_data]\n",
    "print(len(test_data), \"instances\")\n",
    "print(sum([len(sent) for sent in test_data]), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uhOCgG7oz5b",
    "outputId": "c4bcbefe-1493-4371-f1b8-e9b4670d3a9a"
   },
   "outputs": [],
   "source": [
    "print(\"testing tagger...\")\n",
    "preds = []\n",
    "y_test = []\n",
    "for sent in test_data:\n",
    "    sent_preds = [x[1] for x in ct.tag([s[0] for s in sent])]\n",
    "    sent_true = [s[1] for s in sent]\n",
    "    preds.extend(sent_preds)\n",
    "    y_test.extend(sent_true)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NK9gnkPoz5f",
    "outputId": "4ff3b931-5230-4cdb-8490-615de30e2936"
   },
   "outputs": [],
   "source": [
    "# Output the classification report (which you should save each time for comparing your models)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUmd_cFboz5i"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix_heatmap(y_test, preds):\n",
    "    \"\"\"Function to plot a confusion matrix\"\"\"\n",
    "    labels = list(set(y_test))   # get the labels in the y_test\n",
    "    # print(labels)\n",
    "    cm = confusion_matrix(y_test, preds, labels)\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion matrix of the classifier')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels( labels, rotation=45)\n",
    "    ax.set_yticklabels( labels)\n",
    "\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            text = ax.text(j, i, cm[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    #fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ys_TSM6Koz5m",
    "outputId": "3d9a25a2-9cd5-4d2b-e666-1d4911ab49df"
   },
   "outputs": [],
   "source": [
    "confusion_matrix_heatmap(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBR1B1TAoz5u"
   },
   "source": [
    "# Feature experimentation\n",
    "\n",
    "Experiment with different features by further adjusting the `get_features` function, and modifying it to get the best results in terms of `macro average f-score` (i.e. average f-score across all classes) on your 20% development data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We adjust get_feature by adding suffix from 3 to 5, adding prefix to 5, tokenization, and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sv9rg81_oz5v"
   },
   "outputs": [],
   "source": [
    "raw_testing_data = get_raw_data_from_bio_file(\"engtest.bio.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CRF BIO-tag tagger\n",
    "TAGGER_PATH = \"crf_nlu.tagger\"  # path to the tagger- it will save/access the model from here\n",
    "ct = CRFTagger(feature_func=get_features)  # initialize tagger with get_features function\n",
    "\n",
    "print(\"training tagger...\")\n",
    "ct.train(training_data, TAGGER_PATH)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tagger from saved file\n",
    "ct = CRFTagger(feature_func=get_features)  # initialize tagger\n",
    "ct.set_model_file(TAGGER_PATH)  # load model from file\n",
    "\n",
    "# prepare the test data:\n",
    "raw_test_data = get_raw_data_from_bio_file(\"engtest.bio.txt\") \n",
    "test_data = [preProcess(example) for example in raw_test_data]\n",
    "print(len(test_data), \"instances\")\n",
    "print(sum([len(sent) for sent in test_data]), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"testing tagger...\")\n",
    "preds = []\n",
    "y_test = []\n",
    "for sent in test_data:\n",
    "    sent_preds = [x[1] for x in ct.tag([s[0] for s in sent])]\n",
    "    sent_true = [s[1] for s in sent]\n",
    "    preds.extend(sent_preds)\n",
    "    y_test.extend(sent_true)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "percentage = 0.8 #separate the data in 80% of training data and 20% of testing_data\n",
    "data_sample = len(training_data) # find the length of training_data\n",
    "training_sample = int((percentage*data_sample)) # create the number of training data\n",
    "train_dataset = training_data[:training_sample] # create the list which store train_data\n",
    "test_dataset = training_data[training_sample:] # create the list which store test_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"testing tagger...\")\n",
    "# we will train the data by using CRFTagger()\n",
    "predict_result = [] #create list\n",
    "y_predict = []\n",
    "sent_predict_result = []\n",
    "sent_y_predict = []  \n",
    "for i in test_dataset:\n",
    "\n",
    "      # we use the previous code the train the data\n",
    "      sent_predict_result = [k[1] for k in ct.tag([j[0] for j in i])] # we will tag the word(j) by using ct.tag() and store BIO- tag in sent_predict_result\n",
    "      sent_y_predict = [j[1] for j in i] # we store all BIO-tag, which has one line, in sent_y_predict\n",
    "      predict_result.extend(sent_predict_result) # add the data in predict_result \n",
    "      y_predict.extend(sent_y_predict) # add the data in y_predict    \n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_predict,predict_result)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that the macro avg would improve from Q4 by  adding suffix from 3 to 5, adding prefix to 5, tokenization, and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_heatmap(y_predict,predict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_2_CRF_tagging_in_Movie_Queries_Q1-3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
